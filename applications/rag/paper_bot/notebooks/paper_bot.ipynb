{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xl95CheBwmP_",
        "-ejn1v8HhPan"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot for understanding research papers better.\n",
        "\n",
        "Motivation:\n",
        "-  Reading papers can be monotonous sometimes and I have often understood a paper better through reading groups where there is someone presenting and people are asking questions.\n",
        "- Command-R has been specifically trained with grounded generation capabilities.\n",
        "- It can generate responses based on a list of supplied document snippets, and will include grounding spans (citations) in its response indicating the source of the information.\n",
        "- This makes it the perfect LLM for this usecase.\n",
        "\n",
        "Technical Details:\n",
        "- Utilize RAG with the Command-R model to help understand papers better.\n",
        "- Provides a QA interface where a user can ask questions and the model can provide answers along with citations.\n",
        "\n",
        "\n",
        "In this notebook we build a chatbot and a VectorDB. The chatbot leverages the VectorDB to answer a user's questions.\n",
        "\n",
        "References:\n",
        "- Cohere LLM University and Developer Guides."
      ],
      "metadata": {
        "id": "XReDOipmvODY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "xl95CheBwmP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## installs\n",
        "%%capture\n",
        "!pip install cohere langchain pypdf hnswlib"
      ],
      "metadata": {
        "id": "G-XefZU6hiMU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use langchain for parsing pdfs\n",
        "import cohere\n",
        "import hnswlib\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents.base import Document\n",
        "import uuid\n",
        "import urllib.request\n",
        "\n",
        "co = cohere.Client(\"api_key\")"
      ],
      "metadata": {
        "id": "f_YRVtbAhZ52"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Vector DB"
      ],
      "metadata": {
        "id": "-ejn1v8HhPan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorstore:\n",
        "    def __init__(self, paper_url: str) -> None:\n",
        "        \"\"\"Initialize the Vectorstore.\n",
        "\n",
        "        Populates the VectorStore with embeddings and documents.\n",
        "\n",
        "        Args:\n",
        "            paper_url (str): URL to the paper's pdf.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.docs = self.load_docs(paper_url) # can be extended for multiple docs (creating a paper db)\n",
        "        self.docs_len = len(self.docs)\n",
        "        self.docs_embs = []\n",
        "        self.retrieve_top_k = 10 # can be tuned for optimal results\n",
        "        self.rerank_top_k = 3 # can be tuned for optimal results\n",
        "        self.embed_batch_size = 50 # can be optimized for api calls\n",
        "        self.embed()\n",
        "        self.index()\n",
        "\n",
        "    def load_docs(self, paper_url: str) -> list[Document]:\n",
        "        \"\"\" Utilizes the PyPDFLoader to load the paper (pdf) and split it into pages.\n",
        "\n",
        "        Args:\n",
        "            paper_url (str): URL to the paper's pdf.\n",
        "\n",
        "        Returns:\n",
        "            pages (list[Document]): List of pages represented as langchain docs.\n",
        "\n",
        "        \"\"\"\n",
        "        local_path = \"./paper.pdf\"\n",
        "        urllib.request.urlretrieve(paper_url, local_path)\n",
        "        pdf_loader = PyPDFLoader(local_path)\n",
        "        # split pages from pdf\n",
        "        return pdf_loader.load_and_split()\n",
        "\n",
        "    def embed(self) -> None:\n",
        "        \"\"\"Embeds the document chunks using the Cohere Embed API.\"\"\"\n",
        "\n",
        "        for i in range(0, self.docs_len, self.embed_batch_size):\n",
        "\n",
        "            batch = self.docs[i : min(i + self.embed_batch_size, self.docs_len)]\n",
        "            texts = [item.page_content for item in batch]\n",
        "\n",
        "            self.docs_embs.extend(co.embed(texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\").embeddings)\n",
        "\n",
        "        print(\"Embeddings Generated for documents.\")\n",
        "\n",
        "    def index(self) -> None:\n",
        "        \"\"\"Indexes the documents for efficient retrieval.\"\"\"\n",
        "\n",
        "        # NOTE: these params are hard-coded but can be tuned/updated.\n",
        "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "\n",
        "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str) -> list[dict[str, str]]:\n",
        "        \"\"\" Retrieves document chunks based on the given query.\n",
        "\n",
        "        Args:\n",
        "            query (str): The query to retrieve document chunks for.\n",
        "\n",
        "        Returns:\n",
        "            retrieved_docs (list[dict[str, str]]): List of dicts representing the retrieved document chunks, with 'text' and 'metadata'.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dense retrieval\n",
        "        # NOTE: we can do this for other languages as well.\n",
        "        query_emb = co.embed(\n",
        "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
        "        ).embeddings\n",
        "\n",
        "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "\n",
        "        # Reranking\n",
        "        docs_to_rerank = [self.docs[doc_id].page_content for doc_id in doc_ids]\n",
        "\n",
        "        rerank_results = co.rerank(\n",
        "            query=query,\n",
        "            documents=docs_to_rerank,\n",
        "            top_n=self.rerank_top_k,\n",
        "            model=\"rerank-english-v2.0\",\n",
        "        ).results\n",
        "\n",
        "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results]\n",
        "\n",
        "        retrieved_docs = [\n",
        "            {\n",
        "                \"text\": self.docs[doc_id].page_content,\n",
        "                \"metadata\": self.docs[doc_id].metadata,\n",
        "            }\n",
        "            for doc_id in doc_ids_reranked\n",
        "        ]\n",
        "\n",
        "        return retrieved_docs"
      ],
      "metadata": {
        "id": "LaQWnsIEiJ0v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Vectorstore(\"https://arxiv.org/pdf/1706.03762.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhQy9z3Tjp0W",
        "outputId": "9661172c-d659-4f24-fbfd-d5b0b0401404"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings Generated for documents.\n",
            "Indexing complete with 16 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Chatbot"
      ],
      "metadata": {
        "id": "0XpMLtazhRy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PaperBot:\n",
        "    def __init__(self, vectorstore: Vectorstore) -> None:\n",
        "        \"\"\"Initializes an instance of the PaperBot class.\n",
        "\n",
        "        This acts as a chatbot that the user can interact with to discuss research papers (can be extended to any text based pdfs)\n",
        "\n",
        "        Parameters:\n",
        "        vectorstore (Vectorstore): An instance of the Vectorstore class.\n",
        "        \"\"\"\n",
        "        self.vectorstore = vectorstore\n",
        "        self.conversation_id = str(uuid.uuid4()) # required to keep context within the cohere chat.\n",
        "\n",
        "    def run(self) -> None:\n",
        "        \"\"\" Runs the chatbot application.\n",
        "\n",
        "        Opens up an interactive chat window for the user.\n",
        "        Can exit with the input: quit.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            # Manage interactions\n",
        "            message = input(\"USER: \")\n",
        "\n",
        "            if message.lower() == \"quit\":\n",
        "                print(\"Thanks for chatting, talk to you soon!\")\n",
        "                break\n",
        "\n",
        "\n",
        "            # Generate search queries, if any\n",
        "            response = co.chat(message=message, search_queries_only=True)\n",
        "\n",
        "            # If there are search queries, retrieve document chunks and respond\n",
        "            if response.search_queries:\n",
        "                print(\"Retrieving information...\", end=\"\")\n",
        "\n",
        "                # Retrieve document chunks for each query\n",
        "                documents = []\n",
        "                for query in response.search_queries:\n",
        "                    documents.extend(self.vectorstore.retrieve(query.text))\n",
        "\n",
        "                document_contents = [{\"text\": doc[\"text\"]} for doc in documents]\n",
        "\n",
        "                # Use document chunks to respond\n",
        "                response = co.chat_stream(\n",
        "                    message=message,\n",
        "                    model=\"command-r\",\n",
        "                    documents=document_contents,\n",
        "                    conversation_id=self.conversation_id,\n",
        "                )\n",
        "            else:\n",
        "                response = co.chat_stream(\n",
        "                    message=message,\n",
        "                    model=\"command-r\",\n",
        "                    conversation_id=self.conversation_id,\n",
        "                )\n",
        "\n",
        "            # Build chatbot output\n",
        "            print(\"\\nPAPER_BOT:\")\n",
        "            citations = []\n",
        "            cited_documents = []\n",
        "\n",
        "            # Display response\n",
        "            for event in response:\n",
        "                if event.event_type == \"text-generation\":\n",
        "                    print(event.text, end=\"\")\n",
        "                elif event.event_type == \"citation-generation\":\n",
        "                    citations.extend(event.citations)\n",
        "                elif event.event_type == \"search-results\":\n",
        "                    cited_documents = event.documents\n",
        "\n",
        "            # Display citations and source documents\n",
        "            if citations:\n",
        "                print(\"\\n\\nCITATIONS:\")\n",
        "                for citation in citations:\n",
        "                    print(citation)\n",
        "\n",
        "                print(\"\\nDOCUMENTS:\")\n",
        "                for document in cited_documents:\n",
        "                    print(document)\n",
        "\n",
        "            print(f\"\\n{'-'*100}\\n\")"
      ],
      "metadata": {
        "id": "LZSsEPjHz8fn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = PaperBot(vectorstore)"
      ],
      "metadata": {
        "id": "gI1JVIuD1AZI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries:\n",
        "\n",
        "# What are the key contributions of the paper: Attention is all you need?\n",
        "# Could you elaborate more on how parallelism works in the Transformer architecture?\n",
        "# Could you talk about the benchmarks that this new architecture was able to beat?\n",
        "# Could you make me a table of these benchmarks?\n",
        "# Please enhance the table showing the performance of the Transformer as compared to the previous best models. (If generation is off)\n",
        "# In the above table, does a higher score mean better performance?"
      ],
      "metadata": {
        "id": "utTxGuXn125Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf4xUVvG1M0I",
        "outputId": "33013ca0-616d-4ad8-ed29-8d2e974c24a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER: What are the key contributions of the paper: Attention is all you need?\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "The paper \"Attention is all you need\" proposes a new network architecture called the Transformer. The Transformer, unlike existing models, is based solely on attention mechanisms, doing away with recurrence and convolutions. \n",
            "\n",
            "The authors of the paper argue that the Transformer is superior in quality to existing models, whilst being more parallelisable and requiring significantly less training time. To demonstrate the Transformer's efficacy, the authors applied it to two machine translation tasks. On the WMT 2014 English-to-German translation task, the model achieved a 28.4 BLEU score, which was an improvement over the previous best score. Additionally, the Transformer generalises well to other tasks, as shown by its successful application to English constituency parsing.\n",
            "\n",
            "Furthermore, the Transformer has constant computational complexity for each layer, which enables the learning of long-range dependencies. Also, the Transformer allows for more parallelisation due to the inherent sequential nature of recurrent neural networks.\n",
            "\n",
            "CITATIONS:\n",
            "start=53 end=73 text='network architecture' document_ids=['doc_0', 'doc_1']\n",
            "start=85 end=97 text='Transformer.' document_ids=['doc_0', 'doc_1']\n",
            "start=142 end=178 text='based solely on attention mechanisms' document_ids=['doc_0', 'doc_1']\n",
            "start=180 end=224 text='doing away with recurrence and convolutions.' document_ids=['doc_0', 'doc_1']\n",
            "start=282 end=301 text='superior in quality' document_ids=['doc_0']\n",
            "start=335 end=354 text='more parallelisable' document_ids=['doc_0']\n",
            "start=359 end=402 text='requiring significantly less training time.' document_ids=['doc_0']\n",
            "start=472 end=502 text='two machine translation tasks.' document_ids=['doc_0']\n",
            "start=510 end=553 text='WMT 2014 English-to-German translation task' document_ids=['doc_0']\n",
            "start=576 end=591 text='28.4 BLEU score' document_ids=['doc_0']\n",
            "start=606 end=647 text='improvement over the previous best score.' document_ids=['doc_0']\n",
            "start=678 end=709 text='generalises well to other tasks' document_ids=['doc_0']\n",
            "start=727 end=782 text='successful application to English constituency parsing.' document_ids=['doc_0']\n",
            "start=817 end=865 text='constant computational complexity for each layer' document_ids=['doc_2']\n",
            "start=873 end=921 text='enables the learning of long-range dependencies.' document_ids=['doc_2']\n",
            "start=960 end=1043 text='parallelisation due to the inherent sequential nature of recurrent neural networks.' document_ids=['doc_1']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}\n",
            "{'id': 'doc_1', 'text': '1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence'}\n",
            "{'id': 'doc_2', 'text': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you elaborate more on how parallelism works in the Transformer architecture?\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "Sure! The Transformer architecture's parallelism is enabled by the inherent nature of the attention mechanism. In traditional recurrent neural network architectures, computation is factored along the symbol positions of the input and output sequences, which inherently limits parallelisation due to the sequential nature of the networks. \n",
            "\n",
            "The Transformer, however, relies entirely on the attention mechanism, which enables global dependencies between input and output without regard to their distance. This reduces the number of operations required to relate signals from two positions to a constant number, regardless of their distance. As a result, the Transformer can process fixed-length sequences in parallel, making it significantly more parallelisable than previous models based on recurrence or convolutions. \n",
            "\n",
            "This parallelisability also enables faster training of the model, as demonstrated by the authors who trained the Transformer on eight P100 GPUs in twelve hours.\n",
            "\n",
            "CITATIONS:\n",
            "start=90 end=110 text='attention mechanism.' document_ids=['doc_0']\n",
            "start=126 end=150 text='recurrent neural network' document_ids=['doc_0']\n",
            "start=166 end=216 text='computation is factored along the symbol positions' document_ids=['doc_0']\n",
            "start=224 end=250 text='input and output sequences' document_ids=['doc_0']\n",
            "start=269 end=337 text='limits parallelisation due to the sequential nature of the networks.' document_ids=['doc_0']\n",
            "start=366 end=408 text='relies entirely on the attention mechanism' document_ids=['doc_0', 'doc_2']\n",
            "start=424 end=468 text='global dependencies between input and output' document_ids=['doc_0']\n",
            "start=520 end=586 text='number of operations required to relate signals from two positions' document_ids=['doc_0']\n",
            "start=592 end=607 text='constant number' document_ids=['doc_0']\n",
            "start=680 end=714 text='fixed-length sequences in parallel' document_ids=['doc_0']\n",
            "start=740 end=780 text='more parallelisable than previous models' document_ids=['doc_0', 'doc_2']\n",
            "start=790 end=817 text='recurrence or convolutions.' document_ids=['doc_2']\n",
            "start=856 end=884 text='faster training of the model' document_ids=['doc_2']\n",
            "start=921 end=980 text='trained the Transformer on eight P100 GPUs in twelve hours.' document_ids=['doc_2']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': '1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence'}\n",
            "{'id': 'doc_1', 'text': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'}\n",
            "{'id': 'doc_2', 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you talk about the benchmarks that this new architecture was able to beat?\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "The Transformer architecture outperformed several previous models on the WMT 2014 English-to-German translation task. The base model achieved a BLEU score of 27.3, while the big model scored 28.4, which was a state-of-the-art BLEU score. \n",
            "\n",
            "On the WMT 2014 English-to-French translation task, the big model achieved a BLEU score of 41.0, which was also a new single-model state of the art. \n",
            "\n",
            "The Transformer also generalised well to the English constituency parsing task, outperforming most models with the exception of the Recurrent Neural Network Grammar.\n",
            "\n",
            "CITATIONS:\n",
            "start=29 end=65 text='outperformed several previous models' document_ids=['doc_0', 'doc_1']\n",
            "start=73 end=117 text='WMT 2014 English-to-German translation task.' document_ids=['doc_0', 'doc_1']\n",
            "start=122 end=162 text='base model achieved a BLEU score of 27.3' document_ids=['doc_0']\n",
            "start=174 end=195 text='big model scored 28.4' document_ids=['doc_0', 'doc_1']\n",
            "start=209 end=237 text='state-of-the-art BLEU score.' document_ids=['doc_0', 'doc_1']\n",
            "start=247 end=290 text='WMT 2014 English-to-French translation task' document_ids=['doc_0', 'doc_1']\n",
            "start=317 end=335 text='BLEU score of 41.0' document_ids=['doc_0']\n",
            "start=354 end=388 text='new single-model state of the art.' document_ids=['doc_0', 'doc_1']\n",
            "start=412 end=469 text='generalised well to the English constituency parsing task' document_ids=['doc_1']\n",
            "start=471 end=496 text='outperforming most models' document_ids=['doc_1']\n",
            "start=506 end=556 text='exception of the Recurrent Neural Network Grammar.' document_ids=['doc_1']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'}\n",
            "{'id': 'doc_1', 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}\n",
            "{'id': 'doc_2', 'text': 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you make me a table of these benchmarks?\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "Certainly! Here's a table showcasing the benchmarks achieved by the Transformer architecture:\n",
            "\n",
            "| Task | Benchmark Score |\n",
            "|---|---|\n",
            "| WMT 2014 English-to-German | 28.4 BLEU score |\n",
            "| WMT 2014 English-to-French | 41.0 BLEU score |\n",
            "| English Constituency Parsing | 92.7 F1 score (semi-supervised), 91.3 F1 score (WSJ only) |\n",
            "\n",
            "The Transformer outperformed all previous models on the translation tasks, whilst also generalising well to the parsing task.\n",
            "\n",
            "CITATIONS:\n",
            "start=134 end=160 text='WMT 2014 English-to-German' document_ids=['doc_1']\n",
            "start=163 end=172 text='28.4 BLEU' document_ids=['doc_1']\n",
            "start=183 end=209 text='WMT 2014 English-to-French' document_ids=['doc_1']\n",
            "start=212 end=221 text='41.0 BLEU' document_ids=['doc_1']\n",
            "start=232 end=260 text='English Constituency Parsing' document_ids=['doc_0', 'doc_2']\n",
            "start=263 end=294 text='92.7 F1 score (semi-supervised)' document_ids=['doc_0']\n",
            "start=296 end=320 text='91.3 F1 score (WSJ only)' document_ids=['doc_0', 'doc_2']\n",
            "start=340 end=372 text='outperformed all previous models' document_ids=['doc_0', 'doc_1']\n",
            "start=411 end=428 text='generalising well' document_ids=['doc_0']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'}\n",
            "{'id': 'doc_1', 'text': 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'}\n",
            "{'id': 'doc_2', 'text': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Please enhance the table showing the performance of the Transformer as compared to the previous best models.\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "Here's the enhanced table that compares the Transformer's performance with previous models on the WMT 2014 English-to-German translation task.\n",
            "\n",
            "| Model | BLEU Score |\n",
            "|---|---|\n",
            "| ByteNet | 23.75 |\n",
            "| Deep-Att + PosUnk | 39.2 |\n",
            "| GNMT + RL | 24.6 |\n",
            "| ConvS2S | 25.16 |\n",
            "| MoE | 26.03 |\n",
            "| Deep-Att + PosUnk Ensemble | 40.4 |\n",
            "| GNMT + RL Ensemble | 26.30 |\n",
            "| ConvS2S Ensemble | 26.36 |\n",
            "| **Transformer (base model)** | **27.3** |\n",
            "| **Transformer (big model)** | **28.4** |\n",
            "\n",
            "CITATIONS:\n",
            "start=98 end=142 text='WMT 2014 English-to-German translation task.' document_ids=['doc_0']\n",
            "start=146 end=151 text='Model' document_ids=['doc_0']\n",
            "start=154 end=164 text='BLEU Score' document_ids=['doc_0']\n",
            "start=179 end=186 text='ByteNet' document_ids=['doc_0']\n",
            "start=189 end=194 text='23.75' document_ids=['doc_0']\n",
            "start=199 end=216 text='Deep-Att + PosUnk' document_ids=['doc_0']\n",
            "start=219 end=223 text='39.2' document_ids=['doc_0']\n",
            "start=228 end=237 text='GNMT + RL' document_ids=['doc_0']\n",
            "start=240 end=244 text='24.6' document_ids=['doc_0']\n",
            "start=249 end=256 text='ConvS2S' document_ids=['doc_0']\n",
            "start=259 end=264 text='25.16' document_ids=['doc_0']\n",
            "start=269 end=272 text='MoE' document_ids=['doc_0']\n",
            "start=275 end=280 text='26.03' document_ids=['doc_0']\n",
            "start=285 end=311 text='Deep-Att + PosUnk Ensemble' document_ids=['doc_0']\n",
            "start=314 end=318 text='40.4' document_ids=['doc_0']\n",
            "start=323 end=341 text='GNMT + RL Ensemble' document_ids=['doc_0']\n",
            "start=344 end=349 text='26.30' document_ids=['doc_0']\n",
            "start=354 end=370 text='ConvS2S Ensemble' document_ids=['doc_0']\n",
            "start=373 end=378 text='26.36' document_ids=['doc_0']\n",
            "start=385 end=409 text='Transformer (base model)' document_ids=['doc_0']\n",
            "start=416 end=420 text='27.3' document_ids=['doc_0', 'doc_1']\n",
            "start=429 end=452 text='Transformer (big model)' document_ids=['doc_0']\n",
            "start=459 end=463 text='28.4' document_ids=['doc_0', 'doc_1']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'}\n",
            "{'id': 'doc_1', 'text': 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'}\n",
            "{'id': 'doc_2', 'text': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: In the above table, does a higher score mean better performance?\n",
            "Retrieving information...\n",
            "PAPER_BOT:\n",
            "Yes, a higher BLEU score does indeed indicate better performance in the table. The higher scores obtained by the Transformer benchmarks signify the model's superior performance compared to previous ones on the WMT 2014 English-to-German translation task.\n",
            "\n",
            "CITATIONS:\n",
            "start=7 end=24 text='higher BLEU score' document_ids=['doc_0']\n",
            "start=37 end=64 text='indicate better performance' document_ids=['doc_0']\n",
            "start=83 end=124 text='higher scores obtained by the Transformer' document_ids=['doc_0']\n",
            "start=148 end=176 text=\"model's superior performance\" document_ids=['doc_0']\n",
            "start=189 end=204 text='previous models' document_ids=['doc_0']\n",
            "start=212 end=256 text='WMT 2014 English-to-German translation task.' document_ids=['doc_0']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'}\n",
            "{'id': 'doc_1', 'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'}\n",
            "{'id': 'doc_2', 'text': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: quit\n",
            "Thanks for chatting, talk to you soon!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison to Command-R + tool-connector (web search)\n",
        "\n",
        "- Command-R has been specifically trained with conversational tool use capabilities.\n",
        "- Given this it would be useful to compare the RAG approach to the tool-use approach where it can use the search tool without access to the actual paper."
      ],
      "metadata": {
        "id": "_NSQUIHQ4EK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchBot:\n",
        "    def __init__(self, connectors: list[str]):\n",
        "        \"\"\"Initializes an instance of the SearchBot class.\n",
        "\n",
        "        Parameters:\n",
        "        connectors (list[str]): list of connectors for the bot to use.\n",
        "        \"\"\"\n",
        "        self.conversation_id = str(uuid.uuid4())\n",
        "        self.connectors = [cohere.ChatConnector(id=connector) for connector in connectors]\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\" Runs the chatbot application.\n",
        "\n",
        "        Opens up an interactive chat window for the user.\n",
        "        Can exit with the input: quit.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            # Manage interactions\n",
        "            message = input(\"USER: \")\n",
        "\n",
        "            if message.lower() == \"quit\":\n",
        "                print(\"Thanks for chatting, talk to you soon!\")\n",
        "                break\n",
        "\n",
        "\n",
        "            # Generate response\n",
        "            response = co.chat_stream(\n",
        "                    message=message,\n",
        "                    model=\"command-r\",\n",
        "                    conversation_id=self.conversation_id,\n",
        "                    connectors=self.connectors,\n",
        "            )\n",
        "\n",
        "            # Print the chatbot response, citations, and documents\n",
        "            print(\"\\nSEARCH_BOT:\")\n",
        "            citations = []\n",
        "            cited_documents = []\n",
        "\n",
        "            # Display response\n",
        "            for event in response:\n",
        "                if event.event_type == \"text-generation\":\n",
        "                    print(event.text, end=\"\")\n",
        "                elif event.event_type == \"citation-generation\":\n",
        "                    citations.extend(event.citations)\n",
        "                elif event.event_type == \"search-results\":\n",
        "                    cited_documents = event.documents\n",
        "\n",
        "            # Display citations and source documents\n",
        "            if citations:\n",
        "              print(\"\\n\\nCITATIONS:\")\n",
        "              for citation in citations:\n",
        "                print(citation)\n",
        "\n",
        "              print(\"\\nDOCUMENTS:\")\n",
        "              for document in cited_documents:\n",
        "                print({'id': document['id'],\n",
        "                      'snippet': document['snippet'][:50] + '...',\n",
        "                      'title': document['title'],\n",
        "                      'url': document['url']})\n",
        "\n",
        "            print(f\"\\n{'-'*100}\\n\")"
      ],
      "metadata": {
        "id": "daajTLvZ21bn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_bot = SearchBot([\"web-search\"])"
      ],
      "metadata": {
        "id": "WHV5PpOe5efc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries:\n",
        "\n",
        "# What are the key contributions of the paper: Attention is all you need?\n",
        "# Could you elaborate more on how parallelism works in the Transformer architecture?\n",
        "# Could you talk about the benchmarks that this new architecture was able to beat?\n",
        "# Could you make me a table of these benchmarks?\n",
        "# Please enhance the table showing the performance of the Transformer as compared to the previous best models. (If generation is off)\n",
        "# In the above table, does a higher score mean better performance?"
      ],
      "metadata": {
        "id": "pB0COwUW5rli"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_bot.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLFHbMnL5mh4",
        "outputId": "b79047db-f6f1-4388-cbcf-59b8fca635be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER: What are the key contributions of the paper: Attention is all you need?\n",
            "\n",
            "SEARCH_BOT:\n",
            "The paper \"Attention is All You Need\" introduces the Transformer architecture along with its key components. The Transformer is a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions. \n",
            "\n",
            "The paper's main contribution lies in demonstrating the effectiveness of the attention mechanism in NLP tasks, which enables the model to capture long-term dependencies and contextual relationships between words. The Transformer's ability to process input sequences concurrently using self-attention improves training speed and efficiency. \n",
            "\n",
            "Another key insight is the introduction of multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously, capturing complex relationships between the input and output sequences.\n",
            "\n",
            "The Transformer architecture has achieved state-of-the-art results in several NLP tasks, including machine translation, text summarisation and question answering.\n",
            "\n",
            "CITATIONS:\n",
            "start=53 end=77 text='Transformer architecture' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_5', 'web-search_8']\n",
            "start=93 end=108 text='key components.' document_ids=['web-search_8']\n",
            "start=130 end=163 text='novel sequence transduction model' document_ids=['web-search_2', 'web-search_4']\n",
            "start=164 end=202 text='based entirely on attention mechanisms' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_4', 'web-search_8']\n",
            "start=204 end=248 text='dispensing with recurrence and convolutions.' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_4', 'web-search_8']\n",
            "start=289 end=360 text='demonstrating the effectiveness of the attention mechanism in NLP tasks' document_ids=['web-search_2', 'web-search_3']\n",
            "start=389 end=419 text='capture long-term dependencies' document_ids=['web-search_2', 'web-search_3', 'web-search_5']\n",
            "start=424 end=463 text='contextual relationships between words.' document_ids=['web-search_2', 'web-search_3', 'web-search_5']\n",
            "start=482 end=529 text='ability to process input sequences concurrently' document_ids=['web-search_0', 'web-search_1']\n",
            "start=536 end=550 text='self-attention' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_8']\n",
            "start=551 end=590 text='improves training speed and efficiency.' document_ids=['web-search_0', 'web-search_1']\n",
            "start=636 end=656 text='multi-head attention' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_4', 'web-search_8']\n",
            "start=684 end=745 text='focus on different parts of the input sequence simultaneously' document_ids=['web-search_0', 'web-search_2', 'web-search_8']\n",
            "start=747 end=818 text='capturing complex relationships between the input and output sequences.' document_ids=['web-search_2', 'web-search_3']\n",
            "start=862 end=907 text='state-of-the-art results in several NLP tasks' document_ids=['web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_8']\n",
            "start=919 end=938 text='machine translation' document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_8']\n",
            "start=940 end=958 text='text summarisation' document_ids=['web-search_2', 'web-search_3']\n",
            "start=963 end=982 text='question answering.' document_ids=['web-search_2', 'web-search_3']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'web-search_0', 'snippet': 'Skip to primary navigation\\n\\nAPI : Build with DeepA...', 'title': \"AI: A Comprehensive Guide to 'Attention Is All You Need' in Transformers - Deep AI — Leading Generative AI-powered Solutions for Business\", 'url': 'https://deepai.tn/glossary/attention-is-all-you-need-explained/'}\n",
            "{'id': 'web-search_1', 'snippet': 'Sign in to view more content\\n\\nCreate your free acc...', 'title': 'Attention is All You Need', 'url': 'https://www.linkedin.com/pulse/attention-all-you-need-sridevi-bonthu'}\n",
            "{'id': 'web-search_2', 'snippet': '“Attention is All You Need” Summary\\n\\nStable Diffus...', 'title': '“Attention is All You Need” Summary | Medium', 'url': 'https://medium.com/@dminhk/attention-is-all-you-need-summary-6f0437e63a91'}\n",
            "{'id': 'web-search_3', 'snippet': 'Top Competitive Programmers\\n\\nUnsolved Problems in ...', 'title': 'Attention Is All You Need: Paper Summary and Insights', 'url': 'https://iq.opengenus.org/attention-is-all-you-need-summary/'}\n",
            "{'id': 'web-search_4', 'snippet': 'Top 6 Cybersecurity Books from Packt to Accelerate...', 'title': 'Paper in Two minutes: Attention Is All You Need | Packt Hub', 'url': 'https://hub.packtpub.com/paper-in-two-minutes-attention-is-all-you-need/'}\n",
            "{'id': 'web-search_5', 'snippet': 'Sign in to view more content\\n\\nCreate your free acc...', 'title': 'Is Attention is All You Need?', 'url': 'https://www.linkedin.com/pulse/attention-all-you-need-ryan-s-'}\n",
            "{'id': 'web-search_6', 'snippet': 'Is Attention All You Need?\\n\\nOn January 1, 2027, a ...', 'title': 'Is Attention All You Need?', 'url': 'https://www.isattentionallyouneed.com/'}\n",
            "{'id': 'web-search_7', 'snippet': 'Stack Exchange Network\\n\\nStack Exchange network con...', 'title': 'neural networks - What exactly are keys, queries, and values in attention mechanisms? - Cross Validated', 'url': 'https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms'}\n",
            "{'id': 'web-search_8', 'snippet': 'Attention Is All You Need: The Core Idea of the Tr...', 'title': 'Attention Is All You Need: The Core Idea of the Transformer | by Zain ul Abideen | Medium', 'url': 'https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937'}\n",
            "{'id': 'web-search_9', 'snippet': 'Subscribe to the PwC Newsletter ×\\n\\nJoin the commun...', 'title': 'Papers with Code - Attention Is All You Need', 'url': 'https://paperswithcode.com/paper/attention-is-all-you-need'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you elaborate more on how parallelism works in the Transformer architecture?\n",
            "\n",
            "SEARCH_BOT:\n",
            "The Transformer architecture enables parallelism in several ways, allowing for increased computational efficiency. Parallelism is achieved by dividing computations across multiple tensors, which enable concurrent processing. \n",
            "\n",
            "**Data Parallelism**\n",
            "DataParallel (DP) involves replicating the same setup (model, data, etc.) multiple times, where each instance processes a different slice of the full batch of data. The results are synchronised at the end of each training step.\n",
            "\n",
            "**Tensor Parallelism**\n",
            "TensorParallel (TP) involves splitting each tensor into multiple chunks, enabling multiple GPUs to process them in parallel. The partial results are combined at the end.\n",
            "\n",
            "**Pipeline Parallelism**\n",
            "PipelineParallel (PP) involves splitting the model vertically across multiple GPUs, where each GPU processes a different chunk of the pipeline simultaneously.\n",
            "\n",
            "**Zero Redundancy Optimizer**\n",
            "Zero Redundancy Optimizer (ZeRO) also performs sharding of tensors, similar to TP. However, the whole tensor is reconstructed before the forward or backward computation. ZeRO also supports offloading techniques to manage limited GPU memory.\n",
            "\n",
            "**Model Parallelism**\n",
            "This involves dividing the model across multiple GPUs, which allow them to work concurrently. Model parallelism can be further divided into Naive Model Parallelism and Pipeline Model Parallelism. The former involves splitting the model vertically across GPUs, while the latter splits it horizontally.\n",
            "\n",
            "Parallelism in the Transformer architecture also extends to the attention mechanism, a key component of the model. The attention mechanism can be parallelised by partitioning the Key (K), Query (Q) and Value (V) matrices in a column-parallel fashion. Additionally, the multi-head attention operation allows multiple attention heads to be computed simultaneously, further enhancing the parallelisation capabilities of the model.\n",
            "\n",
            "CITATIONS:\n",
            "start=171 end=187 text='multiple tensors' document_ids=['web-search_0', 'web-search_9']\n",
            "start=202 end=224 text='concurrent processing.' document_ids=['web-search_1', 'web-search_4', 'web-search_6', 'web-search_7', 'web-search_8', 'web-search_9']\n",
            "start=229 end=245 text='Data Parallelism' document_ids=['web-search_0', 'web-search_9']\n",
            "start=248 end=260 text='DataParallel' document_ids=['web-search_0', 'web-search_9']\n",
            "start=261 end=265 text='(DP)' document_ids=['web-search_0']\n",
            "start=275 end=301 text='replicating the same setup' document_ids=['web-search_0', 'web-search_9']\n",
            "start=322 end=336 text='multiple times' document_ids=['web-search_0', 'web-search_9']\n",
            "start=344 end=412 text='each instance processes a different slice of the full batch of data.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=417 end=475 text='results are synchronised at the end of each training step.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=479 end=497 text='Tensor Parallelism' document_ids=['web-search_0', 'web-search_9']\n",
            "start=500 end=514 text='TensorParallel' document_ids=['web-search_0', 'web-search_9']\n",
            "start=515 end=519 text='(TP)' document_ids=['web-search_0']\n",
            "start=529 end=571 text='splitting each tensor into multiple chunks' document_ids=['web-search_0', 'web-search_9']\n",
            "start=582 end=624 text='multiple GPUs to process them in parallel.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=649 end=669 text='combined at the end.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=673 end=693 text='Pipeline Parallelism' document_ids=['web-search_0', 'web-search_9']\n",
            "start=696 end=712 text='PipelineParallel' document_ids=['web-search_0', 'web-search_9']\n",
            "start=713 end=717 text='(PP)' document_ids=['web-search_0']\n",
            "start=727 end=778 text='splitting the model vertically across multiple GPUs' document_ids=['web-search_0', 'web-search_9']\n",
            "start=786 end=854 text='each GPU processes a different chunk of the pipeline simultaneously.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=858 end=883 text='Zero Redundancy Optimizer' document_ids=['web-search_0']\n",
            "start=886 end=911 text='Zero Redundancy Optimizer' document_ids=['web-search_0']\n",
            "start=912 end=918 text='(ZeRO)' document_ids=['web-search_0']\n",
            "start=933 end=952 text='sharding of tensors' document_ids=['web-search_0']\n",
            "start=954 end=968 text='similar to TP.' document_ids=['web-search_0', 'web-search_9']\n",
            "start=982 end=1055 text='whole tensor is reconstructed before the forward or backward computation.' document_ids=['web-search_0']\n",
            "start=1075 end=1126 text='offloading techniques to manage limited GPU memory.' document_ids=['web-search_0']\n",
            "start=1130 end=1147 text='Model Parallelism' document_ids=['web-search_0', 'web-search_3']\n",
            "start=1164 end=1203 text='dividing the model across multiple GPUs' document_ids=['web-search_0', 'web-search_3']\n",
            "start=1230 end=1243 text='concurrently.' document_ids=['web-search_0', 'web-search_3']\n",
            "start=1290 end=1313 text='Naive Model Parallelism' document_ids=['web-search_0']\n",
            "start=1318 end=1345 text='Pipeline Model Parallelism.' document_ids=['web-search_0']\n",
            "start=1366 end=1408 text='splitting the model vertically across GPUs' document_ids=['web-search_0']\n",
            "start=1427 end=1450 text='splits it horizontally.' document_ids=['web-search_0']\n",
            "start=1516 end=1535 text='attention mechanism' document_ids=['web-search_6', 'web-search_9']\n",
            "start=1539 end=1552 text='key component' document_ids=['web-search_6', 'web-search_8', 'web-search_9']\n",
            "start=1614 end=1672 text='partitioning the Key (K), Query (Q) and Value (V) matrices' document_ids=['web-search_9']\n",
            "start=1678 end=1702 text='column-parallel fashion.' document_ids=['web-search_9']\n",
            "start=1721 end=1751 text='multi-head attention operation' document_ids=['web-search_6', 'web-search_9']\n",
            "start=1759 end=1813 text='multiple attention heads to be computed simultaneously' document_ids=['web-search_6', 'web-search_9']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'web-search_0', 'snippet': 'Transformers documentation\\n\\nAWS Trainium & Inferen...', 'title': 'Model Parallelism', 'url': 'https://huggingface.co/docs/transformers/v4.15.0/parallelism'}\n",
            "{'id': 'web-search_1', 'snippet': 'Stack Exchange Network\\n\\nStack Exchange network con...', 'title': 'Why people always say the Transformer is parallelizable while the self-attention layer still depends on outputs of all time steps to calculate?', 'url': 'https://ai.stackexchange.com/questions/29903/why-people-always-say-the-transformer-is-parallelizable-while-the-self-attention?rq=1'}\n",
            "{'id': 'web-search_2', 'snippet': 'A Deep Dive Into the Transformer Architecture — Th...', 'title': 'A Deep Dive Into the Transformer Architecture — The Development of Transformer Models | by James Montantes | Towards Data Science', 'url': 'https://towardsdatascience.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0?gi=512c76247709'}\n",
            "{'id': 'web-search_3', 'snippet': 'Model Parallelism using Transformers and PyTorch\\n\\n...', 'title': 'Model Parallelism using Transformers and PyTorch | by Sakthi Ganesh | msakthiganesh | Medium', 'url': 'https://medium.com/msakthiganesh/model-parallelism-using-transformers-and-pytorch-e751cc3e2303'}\n",
            "{'id': 'web-search_4', 'snippet': 'Why you should be concerned about ChatGPT’s weird ...', 'title': 'Machine learning: What is the transformer architecture? - TechTalks', 'url': 'https://bdtechtalks.com/2022/05/02/what-is-the-transformer/'}\n",
            "{'id': 'web-search_5', 'snippet': '...', 'title': 'Arxiv', 'url': 'https://arxiv.org/pdf/1810.12427.pdf'}\n",
            "{'id': 'web-search_6', 'snippet': 'This article relies excessively on references to p...', 'title': 'Transformer (deep learning architecture) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'}\n",
            "{'id': 'web-search_7', 'snippet': 'Skip to main content\\n\\nScan this QR code to downloa...', 'title': \"r/deeplearning on Reddit: Why is it said that the transformer is more parallelizable than RNN's?\", 'url': 'https://www.reddit.com/r/deeplearning/comments/14ad4of/why_is_it_said_that_the_transformer_is_more/'}\n",
            "{'id': 'web-search_8', 'snippet': 'A deep dive into transformers\\n\\nThe transformer arc...', 'title': 'A deep dive into transformers. The transformer architecture is taking… | by Kristof Slechten | Medium', 'url': 'https://kristofsl.medium.com/a-deep-dive-into-transformers-45687c374749'}\n",
            "{'id': 'web-search_9', 'snippet': '↓Skip to main content\\n\\nAnalyzing Parallelization o...', 'title': 'Analyzing Parallelization of Attention · Better Tomorrow with Computer Science', 'url': 'https://insujang.github.io/2022-08-03/analyzing-parallelization-of-attention/'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you talk about the benchmarks that this new architecture was able to beat?\n",
            "\n",
            "SEARCH_BOT:\n",
            "Sure, here's an extended version of the table showcasing the Transformer model's performance against previous models on various benchmarks:\n",
            "\n",
            "| Benchmarks | Previous Models' Performance | Transformer Model's Performance | Performance Improvement |\n",
            "|---|---|---|---|\n",
            "| WMT 2014 English-to-German | - | 28.4 BLEU | N/A |\n",
            "| WMT 2014 English-to-French | - | 41.0 to 41.8 BLEU | N/A |\n",
            "| WMT 2016 English-to-Romanian | - | State-of-the-art results | N/A |\n",
            "| Long ListOps | - | Varies across models | - |\n",
            "| Character-Level Text Classification | - | Performer: 5.7x faster than vanilla Transformer | - |\n",
            "| Character-Level Text Classification | - | Linformer: 9.58x less memory than vanilla Transformer | - |\n",
            "| Image Classification on Sequences of Pixels | - | Varies across models | - |\n",
            "| Pathfinder and Pathfinder-X | - | All models' performance was no better than random chance | - |\n",
            "| QA Benchmarks (SQuAD, TriviaQA, HotpotQA) | - | State-of-the-art results | - |\n",
            "| NLI Benchmarks (SNLI, MultiNLI, XNLI) | - | State-of-the-art results | - |\n",
            "\n",
            "As mentioned earlier, the Transformer architecture's ability to process input sequences in parallel improves training speed and efficiency. The parallelization of the attention mechanism, multi-head attention, and other components enable the model to achieve these superior benchmark results.\n",
            "\n",
            "CITATIONS:\n",
            "start=267 end=293 text='WMT 2014 English-to-German' document_ids=['web-search_3']\n",
            "start=300 end=309 text='28.4 BLEU' document_ids=['web-search_3']\n",
            "start=320 end=346 text='WMT 2014 English-to-French' document_ids=['web-search_3']\n",
            "start=353 end=370 text='41.0 to 41.8 BLEU' document_ids=['web-search_3']\n",
            "start=381 end=409 text='WMT 2016 English-to-Romanian' document_ids=['web-search_3']\n",
            "start=416 end=440 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=451 end=463 text='Long ListOps' document_ids=['web-search_4', 'web-search_8']\n",
            "start=470 end=490 text='Varies across models' document_ids=['web-search_4']\n",
            "start=499 end=534 text='Character-Level Text Classification' document_ids=['web-search_4', 'web-search_8']\n",
            "start=541 end=588 text='Performer: 5.7x faster than vanilla Transformer' document_ids=['web-search_4']\n",
            "start=597 end=632 text='Character-Level Text Classification' document_ids=['web-search_4', 'web-search_8']\n",
            "start=639 end=692 text='Linformer: 9.58x less memory than vanilla Transformer' document_ids=['web-search_4']\n",
            "start=701 end=744 text='Image Classification on Sequences of Pixels' document_ids=['web-search_4', 'web-search_8']\n",
            "start=751 end=771 text='Varies across models' document_ids=['web-search_4']\n",
            "start=780 end=807 text='Pathfinder and Pathfinder-X' document_ids=['web-search_4', 'web-search_8']\n",
            "start=814 end=870 text=\"All models' performance was no better than random chance\" document_ids=['web-search_4']\n",
            "start=879 end=920 text='QA Benchmarks (SQuAD, TriviaQA, HotpotQA)' document_ids=['web-search_3']\n",
            "start=927 end=951 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=960 end=997 text='NLI Benchmarks (SNLI, MultiNLI, XNLI)' document_ids=['web-search_3']\n",
            "start=1004 end=1028 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=1180 end=1222 text='parallelization of the attention mechanism' document_ids=['web-search_1']\n",
            "start=1224 end=1244 text='multi-head attention' document_ids=['web-search_1']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'web-search_0', 'snippet': 'Transformers documentation\\n\\nAWS Trainium & Inferen...', 'title': 'Benchmarks', 'url': 'https://huggingface.co/docs/transformers/benchmarks'}\n",
            "{'id': 'web-search_1', 'snippet': 'This article relies excessively on references to p...', 'title': 'Transformer (deep learning architecture) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'}\n",
            "{'id': 'web-search_2', 'snippet': 'Search or jump to...\\n\\nSearch code, repositories, u...', 'title': 'GitHub - mli/transformers-benchmarks: real Transformer TeraFLOPS on various GPUs', 'url': 'https://github.com/mli/transformers-benchmarks'}\n",
            "{'id': 'web-search_3', 'snippet': 'Full body humans in context\\n\\nGenerate fully annota...', 'title': 'What Is the Transformer Architecture and How Does It Work?', 'url': 'https://datagen.tech/guides/computer-vision/transformer-architecture/'}\n",
            "{'id': 'web-search_4', 'snippet': '🌟 New Course! Enroll in Knowledge Graphs for RAG\\n\\n...', 'title': 'A Benchmark for Comparing Different AI Transformers', 'url': 'https://www.deeplearning.ai/the-batch/transformer-variants-head-to-head/'}\n",
            "{'id': 'web-search_5', 'snippet': 'The Best Comprehensive Guide to Understanding Tran...', 'title': 'The Best Comprehensive Guide to Understanding Transformer Architectures in 2023 | by Diego Bonilla | Medium', 'url': 'https://medium.com/@diegobonila/the-best-comprehensive-guide-to-understanding-transformer-architectures-in-2023-406e90b85edd'}\n",
            "{'id': 'web-search_6', 'snippet': 'ⓘ You are viewing legacy docs. Go to latest docume...', 'title': 'Benchmarks — transformers 4.5.0.dev0 documentation', 'url': 'https://huggingface.co/transformers/v4.5.1/benchmarks.html'}\n",
            "{'id': 'web-search_7', 'snippet': 'Skip to main content\\n\\nWe gratefully acknowledge su...', 'title': '[2102.06621] Optimizing Inference Performance of Transformers on CPUs', 'url': 'https://arxiv.org/abs/2102.06621'}\n",
            "{'id': 'web-search_8', 'snippet': 'Open Peer Review. Open Publishing. Open Access. Op...', 'title': 'Long Range Arena : A Benchmark for Efficient Transformers | OpenReview', 'url': 'https://openreview.net/forum?id=qVyeW-grC2k'}\n",
            "{'id': 'web-search_9', 'snippet': 'ⓘ You are viewing legacy docs. Go to latest docume...', 'title': 'Benchmarks — transformers 2.5.1 documentation', 'url': 'https://huggingface.co/transformers/v2.5.1/benchmarks.html'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: Could you make me a table of these benchmarks?\n",
            "\n",
            "SEARCH_BOT:\n",
            "Sure, here's the table showcasing the performance of the Transformer architecture across various benchmarks. \n",
            "\n",
            "| Benchmarks | Performance Metric | Results |\n",
            "|---|---|---|\n",
            "| WMT 2014 English-to-German Translation Task | BLEU Score | 28.4 or higher |\n",
            "| WMT 16 English-to-Romanian Translation Task | BLEU Score | State-of-the-art results |\n",
            "| Long ListOps | Accuracy | Varies across models |\n",
            "| Character-Level Text Classification | Accuracy | Performer: 5.7x faster than Vanilla Transformer <br> Linformer: 9.58x less memory than Vanilla Transformer |\n",
            "| Character-Level Document Retrieval | - | Not Available |\n",
            "| Image Classification on Sequences of Pixels | Accuracy | Varies across models |\n",
            "| Pathfinder and Pathfinder-X | Accuracy | All models performed no better than random chance |\n",
            "| SQuAD | - | State-of-the-art results |\n",
            "| TriviaQA | - | State-of-the-art results |\n",
            "| HotpotQA | - | State-of-the-art results |\n",
            "| SNLI | - | State-of-the-art results |\n",
            "| MultiNLI | - | State-of-the-art results |\n",
            "| XNLI | - | State-of-the-art results |\n",
            "| Long Range Arena | Efficiency | Performer: Best overall <br> Linformer: Least memory usage |\n",
            "\n",
            "CITATIONS:\n",
            "start=173 end=199 text='WMT 2014 English-to-German' document_ids=['web-search_3']\n",
            "start=200 end=216 text='Translation Task' document_ids=['web-search_3', 'web-search_8']\n",
            "start=219 end=229 text='BLEU Score' document_ids=['web-search_3']\n",
            "start=232 end=236 text='28.4' document_ids=['web-search_3']\n",
            "start=237 end=246 text='or higher' document_ids=['web-search_8']\n",
            "start=251 end=277 text='WMT 16 English-to-Romanian' document_ids=['web-search_3']\n",
            "start=278 end=294 text='Translation Task' document_ids=['web-search_3', 'web-search_8']\n",
            "start=297 end=307 text='BLEU Score' document_ids=['web-search_3']\n",
            "start=310 end=334 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=339 end=351 text='Long ListOps' document_ids=['web-search_4']\n",
            "start=354 end=362 text='Accuracy' document_ids=['web-search_4']\n",
            "start=365 end=385 text='Varies across models' document_ids=['web-search_4']\n",
            "start=390 end=425 text='Character-Level Text Classification' document_ids=['web-search_4']\n",
            "start=428 end=436 text='Accuracy' document_ids=['web-search_4']\n",
            "start=439 end=486 text='Performer: 5.7x faster than Vanilla Transformer' document_ids=['web-search_4']\n",
            "start=492 end=545 text='Linformer: 9.58x less memory than Vanilla Transformer' document_ids=['web-search_4']\n",
            "start=550 end=584 text='Character-Level Document Retrieval' document_ids=['web-search_4']\n",
            "start=609 end=652 text='Image Classification on Sequences of Pixels' document_ids=['web-search_4']\n",
            "start=655 end=663 text='Accuracy' document_ids=['web-search_4']\n",
            "start=666 end=686 text='Varies across models' document_ids=['web-search_4']\n",
            "start=691 end=718 text='Pathfinder and Pathfinder-X' document_ids=['web-search_4']\n",
            "start=721 end=729 text='Accuracy' document_ids=['web-search_4']\n",
            "start=732 end=781 text='All models performed no better than random chance' document_ids=['web-search_4']\n",
            "start=786 end=791 text='SQuAD' document_ids=['web-search_3']\n",
            "start=798 end=822 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=827 end=835 text='TriviaQA' document_ids=['web-search_3']\n",
            "start=842 end=866 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=871 end=879 text='HotpotQA' document_ids=['web-search_3']\n",
            "start=886 end=910 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=915 end=919 text='SNLI' document_ids=['web-search_3']\n",
            "start=926 end=950 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=955 end=963 text='MultiNLI' document_ids=['web-search_3']\n",
            "start=970 end=994 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=999 end=1003 text='XNLI' document_ids=['web-search_3']\n",
            "start=1010 end=1034 text='State-of-the-art results' document_ids=['web-search_3']\n",
            "start=1039 end=1055 text='Long Range Arena' document_ids=['web-search_4', 'web-search_8']\n",
            "start=1058 end=1068 text='Efficiency' document_ids=['web-search_8']\n",
            "start=1071 end=1094 text='Performer: Best overall' document_ids=['web-search_4']\n",
            "start=1100 end=1129 text='Linformer: Least memory usage' document_ids=['web-search_4']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'web-search_0', 'snippet': 'Transformers documentation\\n\\nAWS Trainium & Inferen...', 'title': 'Benchmarks', 'url': 'https://huggingface.co/docs/transformers/benchmarks'}\n",
            "{'id': 'web-search_1', 'snippet': 'This article relies excessively on references to p...', 'title': 'Transformer (deep learning architecture) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'}\n",
            "{'id': 'web-search_2', 'snippet': 'Search or jump to...\\n\\nSearch code, repositories, u...', 'title': 'GitHub - mli/transformers-benchmarks: real Transformer TeraFLOPS on various GPUs', 'url': 'https://github.com/mli/transformers-benchmarks'}\n",
            "{'id': 'web-search_3', 'snippet': 'Full body humans in context\\n\\nGenerate fully annota...', 'title': 'What Is the Transformer Architecture and How Does It Work?', 'url': 'https://datagen.tech/guides/computer-vision/transformer-architecture/'}\n",
            "{'id': 'web-search_4', 'snippet': '🌟 New Course! Enroll in Knowledge Graphs for RAG\\n\\n...', 'title': 'A Benchmark for Comparing Different AI Transformers', 'url': 'https://www.deeplearning.ai/the-batch/transformer-variants-head-to-head/'}\n",
            "{'id': 'web-search_5', 'snippet': 'The Best Comprehensive Guide to Understanding Tran...', 'title': 'The Best Comprehensive Guide to Understanding Transformer Architectures in 2023 | by Diego Bonilla | Medium', 'url': 'https://medium.com/@diegobonila/the-best-comprehensive-guide-to-understanding-transformer-architectures-in-2023-406e90b85edd'}\n",
            "{'id': 'web-search_6', 'snippet': 'ⓘ You are viewing legacy docs. Go to latest docume...', 'title': 'Benchmarks — transformers 4.5.0.dev0 documentation', 'url': 'https://huggingface.co/transformers/v4.5.1/benchmarks.html'}\n",
            "{'id': 'web-search_7', 'snippet': 'Skip to main content\\n\\nWe gratefully acknowledge su...', 'title': '[2102.06621] Optimizing Inference Performance of Transformers on CPUs', 'url': 'https://arxiv.org/abs/2102.06621'}\n",
            "{'id': 'web-search_8', 'snippet': 'Open Peer Review. Open Publishing. Open Access. Op...', 'title': 'Long Range Arena : A Benchmark for Efficient Transformers | OpenReview', 'url': 'https://openreview.net/forum?id=qVyeW-grC2k'}\n",
            "{'id': 'web-search_9', 'snippet': 'ⓘ You are viewing legacy docs. Go to latest docume...', 'title': 'Benchmarks — transformers 2.5.1 documentation', 'url': 'https://huggingface.co/transformers/v2.5.1/benchmarks.html'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: In the above table, does a higher score mean better performance?\n",
            "\n",
            "SEARCH_BOT:\n",
            "Yes, in most cases, a higher score in the benchmarks indicates better performance or efficiency. For instance, in the Transformer model's benchmarks, a higher BLEU score implies better machine translation quality. Similarly, in the context of SD cards, a higher write speed or Class rating implies better performance.\n",
            "\n",
            "However, it is important to note that some benchmark scores are not directly comparable. For example, the scores in attention benchmarks vary significantly based on the methodologies employed. Moreover, some benchmark scores, such as those for smartphones, may not always translate into real-world performance due to differences in usage conditions.\n",
            "\n",
            "CITATIONS:\n",
            "start=22 end=34 text='higher score' document_ids=['web-search_1', 'web-search_9', 'web-search_16', 'web-search_17', 'web-search_19']\n",
            "start=63 end=96 text='better performance or efficiency.' document_ids=['web-search_1', 'web-search_9', 'web-search_16', 'web-search_17', 'web-search_19']\n",
            "start=152 end=169 text='higher BLEU score' document_ids=['web-search_16']\n",
            "start=178 end=213 text='better machine translation quality.' document_ids=['web-search_16']\n",
            "start=243 end=251 text='SD cards' document_ids=['web-search_13']\n",
            "start=255 end=273 text='higher write speed' document_ids=['web-search_13']\n",
            "start=277 end=289 text='Class rating' document_ids=['web-search_13']\n",
            "start=298 end=317 text='better performance.' document_ids=['web-search_13']\n",
            "start=383 end=407 text='not directly comparable.' document_ids=['web-search_15']\n",
            "start=435 end=455 text='attention benchmarks' document_ids=['web-search_15']\n",
            "start=488 end=511 text='methodologies employed.' document_ids=['web-search_15']\n",
            "start=563 end=574 text='smartphones' document_ids=['web-search_13']\n",
            "start=606 end=628 text='real-world performance' document_ids=['web-search_13']\n",
            "start=636 end=668 text='differences in usage conditions.' document_ids=['web-search_13']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'web-search_0', 'snippet': 'Something went wrong. Wait a moment and try again....', 'title': 'Do higher benchmark scores for PC processors mean better performance than lower benchmark score processors? - Quora', 'url': 'https://www.quora.com/Do-higher-benchmark-scores-for-PC-processors-mean-better-performance-than-lower-benchmark-score-processors'}\n",
            "{'id': 'web-search_1', 'snippet': 'A guide to benchmarks and benchmark scores\\n\\nHow to...', 'title': 'A Guide to Benchmarks and Benchmark Scores', 'url': 'https://benchmarks.ul.com/guide-to-scores'}\n",
            "{'id': 'web-search_2', 'snippet': 'Skip To Main Content\\n\\nThe browser version you are ...', 'title': 'How to Read and Understand CPU Benchmarks - Intel', 'url': 'https://www.intel.com/content/www/us/en/gaming/resources/read-cpu-benchmarks.html'}\n",
            "{'id': 'web-search_3', 'snippet': 'Transformers documentation\\n\\nPerformance and Scalab...', 'title': 'Performance and Scalability: How To Fit a Bigger Model and Train It Faster', 'url': 'https://huggingface.co/docs/transformers/v4.18.0/en/performance'}\n",
            "{'id': 'web-search_4', 'snippet': 'Search or jump to...\\n\\nSearch code, repositories, u...', 'title': 'GitHub - mli/transformers-benchmarks: real Transformer TeraFLOPS on various GPUs', 'url': 'https://github.com/mli/transformers-benchmarks'}\n",
            "{'id': 'web-search_5', 'snippet': 'Transformers documentation\\n\\nAWS Trainium & Inferen...', 'title': 'Benchmarks', 'url': 'https://huggingface.co/docs/transformers/benchmarks'}\n",
            "{'id': 'web-search_6', 'snippet': 'Electricity Today Magazine Intelligent Power Today...', 'title': 'Transformer RatingsÂ | The Electricity Forum', 'url': 'https://www.electricityforum.com/iep/electrical-transformers/transformer-ratings'}\n",
            "{'id': 'web-search_7', 'snippet': 'Skip to Main Content\\n\\nView on Facebook Page (Opens...', 'title': 'How Benchmarks Work and When You Should Pay Attention to Them | Lifehacker', 'url': 'https://lifehacker.com/how-benchmarks-work-and-when-you-should-pay-attention-t-1792579167'}\n",
            "{'id': 'web-search_8', 'snippet': 'Speeding Up Transformer Training and Inference By ...', 'title': 'Speeding Up Transformer Training and Inference By Increasing Model Size – The Berkeley Artificial Intelligence Research Blog', 'url': 'https://bair.berkeley.edu/blog/2020/03/05/compress/'}\n",
            "{'id': 'web-search_9', 'snippet': 'Business and Education\\n\\nKlarna and Paypal Credit\\n\\n...', 'title': 'What is a Geekbench 5 Score? | Benchmark List | Hoxton Macs', 'url': 'https://www.hoxtonmacs.co.uk/blogs/news/what-is-a-geekbench-5-score'}\n",
            "{'id': 'web-search_10', 'snippet': 'Subscribe to the PwC Newsletter ×\\n\\nJoin the commun...', 'title': 'Papers with Code - Attention Is All You Need', 'url': 'https://paperswithcode.com/paper/attention-is-all-you-need'}\n",
            "{'id': 'web-search_11', 'snippet': 'Attention metrics: the news KPIs you need to perfo...', 'title': 'Attention metrics: the news KPIs you need to perform', 'url': 'https://en.gamned.com/blog/attention-metrics'}\n",
            "{'id': 'web-search_12', 'snippet': 'Attention is all you need: understanding with exam...', 'title': 'Attention is all you need: understanding with example | by Mehul Gupta | Data Science in your pocket | Medium', 'url': 'https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767'}\n",
            "{'id': 'web-search_13', 'snippet': 'Skip to Main Content\\n\\nView on Facebook Page (Opens...', 'title': 'How Benchmarks Work and When You Should Pay Attention to Them | Lifehacker', 'url': 'https://lifehacker.com/how-benchmarks-work-and-when-you-should-pay-attention-t-1792579167'}\n",
            "{'id': 'web-search_14', 'snippet': 'Brand Safety and Suitability\\n\\nMeasurement & Analyt...', 'title': 'DoubleVerify Launches First-of-its-Kind Global Attention Benchmark Report - DoubleVerify', 'url': 'https://doubleverify.com/newsroom/doubleverify-launches-first-of-its-kind-global-attention-benchmark-report/'}\n",
            "{'id': 'web-search_15', 'snippet': 'Attention Metrics: How to Accurately Measure Ad Pe...', 'title': 'Attention Metrics: How to Accurately Measure Ad Performance Beyond Viewability – Outbrain', 'url': 'https://www.outbrain.com/blog/attention-metrics/'}\n",
            "{'id': 'web-search_16', 'snippet': 'Attention is All You Need\\n\\nDownload Google Scholar...', 'title': 'Attention is All You Need', 'url': 'https://research.google/pubs/attention-is-all-you-need/'}\n",
            "{'id': 'web-search_17', 'snippet': 'Transformer Architecture: Attention is all you nee...', 'title': 'Transformer Architecture: Attention is all you need! | by Vedanth Venkatesh | Medium', 'url': 'https://medium.com/@dogoodwithdata/transformer-architecture-attention-is-all-you-need-d25ac634c68b'}\n",
            "{'id': 'web-search_18', 'snippet': 'Attention Is All You Need: The Core Idea of the Tr...', 'title': 'Attention Is All You Need: The Core Idea of the Transformer | by Zain ul Abideen | Medium', 'url': 'https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937'}\n",
            "{'id': 'web-search_19', 'snippet': 'MachineLearningMastery.com Making developers aweso...', 'title': 'The Transformer Attention Mechanism - MachineLearningMastery.com', 'url': 'https://machinelearningmastery.com/the-transformer-attention-mechanism/'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USER: quit\n",
            "Thanks for chatting, talk to you soon!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "- When comparing the PaperBot to the SearchBot we notice that the SearchBot has access to a lot more than the paper when generating answers.\n",
        "- If we peruse the citations we often see the model using the actual paper along with various blog articles, etc.\n",
        "- This results in a variety in the answers but can also lead to noise.\n",
        "\n",
        "Eg:\n",
        "    - When talking about parallelism it starts explaining what data/model parallelism means instead of sticking to the context of the paper.\n",
        "    - Since it was connected to search it grabbed results that came after the paper was published when generating the table.\n",
        "\n",
        "\n",
        "\n",
        "Overally the results are fascinating because the grounding in data is amazing.\n",
        "We can see that the model is highly optimized for RAG applications as well as tool-use.\n",
        "The noise can be managed via prompt tuning in my opinion."
      ],
      "metadata": {
        "id": "AX4Tx9k66oht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Potential Next Steps/Extensions\n",
        "\n",
        "- This can easily extend to multiple languages (both the search and pdf use-case)\n",
        "- The RAG application could be enhanced to use a paper-database of some sort so that the user doesn't need to provide the pdf url everytime.\n",
        "- The tool-use could be extended to incorporate a variety of connectors supported by cohere.\n",
        "Eg:\n",
        "    - Daily paper discussions on slack with your teammates.\n",
        "    - Generate presentations or articles using medium.\n",
        "    - Create a paper knowledge base in notion.\n",
        "    - etc."
      ],
      "metadata": {
        "id": "VRnG6A7A7MWQ"
      }
    }
  ]
}